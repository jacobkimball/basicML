{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "naval-train",
   "metadata": {},
   "source": [
    "## Regression: starting at the very beginning\n",
    "\n",
    "Remember that although the following examples only have 2 or 3 dimensions, real problems often have MANY more. It is difficult to visualize relationships between multiple features in higher dimensions. If you are interested in exploring these relationships, I suggest you learn more about dimensionality reduction techniques (not covered here). \n",
    "\n",
    "### Linear regression simplest example: Ordinary Least Squares \n",
    "\n",
    "Additional topics covered: Error metrics, Standardization, Feature Importance\n",
    "\n",
    "Let's start by importing all important packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this one's for math\n",
    "\n",
    "import matplotlib.pyplot as plt # this one's for plotting figures \n",
    "from mpl_toolkits.mplot3d import Axes3D # allows for 3D plots \n",
    "\n",
    "import pandas as pd # this one's for manipulating data files \n",
    "\n",
    "import sklearn # this is where all the cool machine learning stuff comes in \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-morocco",
   "metadata": {},
   "source": [
    "Now I'm going to generate some data to work with, and force some relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data for the simplest case \n",
    "N = 50 # number of examples\n",
    "feature1 = np.random.uniform(0,100,N)\n",
    "feature2 = np.random.uniform(0,1,N)\n",
    "X = np.array([feature1,feature2]).transpose()\n",
    "y = feature1 + 15 # NOTICE THAT WE ARE ONLY DEPENDENT ON ONE FEATURE AND ADDING A FIXED OFFSET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-theta",
   "metadata": {},
   "source": [
    "Now we split the data into training and testing sets. Ordinary least squares doesn't have any hyperparameters. Models with hyperparameters will require additional cross validation for hyperparameter tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size  = 0.2 , random_state=1)\n",
    "\n",
    "# format the arrays so that they will work with the algorithms properly\n",
    "X_train.reshape(-1,1)\n",
    "X_test.reshape(-1,1)\n",
    "y_train.reshape(-1,1)\n",
    "y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "plt.title('Perfect world data distribution')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-motion",
   "metadata": {},
   "source": [
    "Now that we've seen the data, let's fit a line:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "# fit model, plot line in red, calculate RMSE and R^2\n",
    "lin_reg = LinearRegression().fit(X_train, y_train) # the default parameters automatically check for an offset \n",
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "xx = np.linspace(0, 100, 1000)\n",
    "yy = w[0]*xx+b # fit line from model (feature 1 only)\n",
    "plt.plot(xx,yy,c='red')\n",
    "plt.title('Perfect data, perfect fit')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-assumption",
   "metadata": {},
   "source": [
    "To evaluate the quality of this fit, we calculate R$^{2}$ and RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "R2_train = r2_score(y_train,y_pred_train)\n",
    "RMSE_train = mean_squared_error(y_train, y_pred_train, squared=False) \n",
    "print('The R\\u00b2 value for this model on the training data is',R2_train)\n",
    "print('The RMSE value for this model on the training data is',RMSE_train)\n",
    "\n",
    "# add test data to plot in different color, calculate RMSE and R^2 \n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "R2_test = r2_score(y_test, y_pred_test) \n",
    "RMSE_test = mean_squared_error(y_test, y_pred_test, squared=False) \n",
    "print('\\nThe R\\u00b2 value for this model on the test data is',R2_test)\n",
    "print('The RMSE value for this model on the test data is',RMSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-context",
   "metadata": {},
   "source": [
    "***\n",
    "Now let's add a little noise and see what happens to R$^{2}$ and RMSE. Change noise_level and see what changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we add noise and the performance goes down\n",
    "N = 100\n",
    "noise_level = 20\n",
    "\n",
    "# generate random data for the simplest case \n",
    "feature1 = np.random.uniform(0,100,N)\n",
    "feature2 = np.random.uniform(0,1,N)\n",
    "X = np.array([feature1,feature2]).transpose()\n",
    "y_uni = feature1 + np.random.uniform(-noise_level,noise_level,N) # SINGLE FEATURE, UNIFORM RANDOM OFFSET \n",
    "\n",
    "# split training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_uni, test_size  = 0.2 , random_state=1)\n",
    "\n",
    "# format the arrays so that they will work with the algorithms properly\n",
    "X_train.reshape(-1,1)\n",
    "X_test.reshape(-1,1)\n",
    "y_train.reshape(-1,1)\n",
    "y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "plt.title('Noisier data distribution')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()\n",
    "\n",
    "# fit model, plot line in red, calculate RMSE and R^2\n",
    "lin_reg = LinearRegression().fit(X_train, y_train) # the default parameters automatically check for an offset \n",
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "xx = np.linspace(0, 100, 1000)\n",
    "yy = w[0]*xx+b # fit line from model (feature 1 only)\n",
    "plt.plot(xx,yy,c='red')\n",
    "plt.title('Fitting to noisier data')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate error stats on the model\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "R2_train = r2_score(y_train,y_pred_train)\n",
    "RMSE_train = mean_squared_error(y_train, y_pred_train, squared=False) \n",
    "print('The R\\u00b2 value for this model on the training data is',R2_train)\n",
    "print('The RMSE value for this model on the training data is',RMSE_train)\n",
    "\n",
    "# add test data to plot in different color, calculate RMSE and R^2 \n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "R2_test = r2_score(y_test, y_pred_test) \n",
    "RMSE_test = mean_squared_error(y_test, y_pred_test, squared=False) \n",
    "print('\\nThe R\\u00b2 value for this model on the test data is',R2_test)\n",
    "print('The RMSE value for this model on the test data is',RMSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-friend",
   "metadata": {},
   "source": [
    "***\n",
    "Linear Regression won't work well on nonlinear distributions, unless you engineer nonlinear features. If you think your use case requires nonlinearity, I recommend using a different model that allows for nonlinear combinations. \n",
    "\n",
    "Let's take a look at trying to fit a nonlinear distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "N = 100\n",
    "noise_level = 500\n",
    "\n",
    "# generate random data for the simplest case \n",
    "feature1 = np.random.uniform(0,100,N)\n",
    "feature2 = np.random.uniform(0,1,N)\n",
    "X = np.array([feature1,feature2]).transpose()\n",
    "y_uni = feature1**3 + np.random.uniform(-noise_level,noise_level,N) # SINGLE FEATURE, UNIFORM RANDOM OFFSET \n",
    "\n",
    "# split training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_uni, test_size  = 0.2 , random_state=1)\n",
    "\n",
    "# format the arrays so that they will work with the algorithms properly\n",
    "X_train.reshape(-1,1)\n",
    "X_test.reshape(-1,1)\n",
    "y_train.reshape(-1,1)\n",
    "y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "plt.title('Nonlinear data distribution')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()\n",
    "\n",
    "# fit model, plot line in red, calculate RMSE and R^2\n",
    "lin_reg = LinearRegression().fit(X_train, y_train) # the default parameters automatically check for an offset \n",
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "\n",
    "fig = plt.figure(); \n",
    "cm = plt.cm.get_cmap('jet') # set the colormap\n",
    "pl1 = plt.scatter(X_train[:,0],y_train,c = 'black',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "pl2 = plt.scatter(X_test[:,0],y_test,c = 'green',s=30,cmap = cm, alpha = 0.7,edgecolors='none')\n",
    "xx = np.linspace(0, 100, 1000)\n",
    "yy = w[0]*xx+b # fit line from model (feature 1 only)\n",
    "plt.plot(xx,yy,c='red')\n",
    "plt.title('Fitting to nonlinear data')\n",
    "plt.xlabel('feature 1',fontsize=12)\n",
    "plt.ylabel('measured output',fontsize=12)\n",
    "# plt.colorbar(pl)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate error stats on the model\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "R2_train = r2_score(y_train,y_pred_train)\n",
    "RMSE_train = mean_squared_error(y_train, y_pred_train, squared=False) \n",
    "print('The R\\u00b2 value for this model on the training data is',R2_train)\n",
    "print('The RMSE value for this model on the training data is',RMSE_train)\n",
    "\n",
    "# add test data to plot in different color, calculate RMSE and R^2 \n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "R2_test = r2_score(y_test, y_pred_test) \n",
    "RMSE_test = mean_squared_error(y_test, y_pred_test, squared=False) \n",
    "print('\\nThe R\\u00b2 value for this model on the test data is',R2_test)\n",
    "print('The RMSE value for this model on the test data is',RMSE_test)\n",
    "\n",
    "\n",
    "\n",
    "print('\\nImportant things to notice:')\n",
    "print('\\t1. The R\\u00b2 value still looks pretty good in this simple case')\n",
    "print('\\t2. The RMSE is always going to be domain specific. It does not fall within a specified range')\n",
    "print('\\t   like R\\u00b2 from 0 to 1. You will need to think about and specify what makes a \"good\" RMSE')\n",
    "print('\\t   value when reporting your results.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-construction",
   "metadata": {},
   "source": [
    "***\n",
    "Let's add another dimension and pretend that we're trying to estimate blood pressure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "N = 100\n",
    "noise_level = 5\n",
    "\n",
    "# generate random data for the simplest case \n",
    "feature1 = np.random.uniform(0,100,N) # represents patient's age \n",
    "feature2 = np.random.uniform(1000,5000,N) # represents daily salt intake in milligrams\n",
    "X = np.array([feature1,feature2]).transpose()\n",
    "Fake_BP = 0.5*feature1+1.75*np.sqrt(feature2)+np.random.uniform(0,noise_level,N) # SINGLE FEATURE, UNIFORM RANDOM OFFSET \n",
    "\n",
    "# split here to another cell, insert commentary on how to split training and testing sets. \n",
    "# hint that it will get more complex with hyperparameters\n",
    "\n",
    "# split training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Fake_BP, test_size  = 0.2 , random_state=1)\n",
    "\n",
    "# format the arrays so that they will work with the algorithms properly\n",
    "X_train.reshape(-1,1)\n",
    "X_test.reshape(-1,1)\n",
    "y_train.reshape(-1,1)\n",
    "y_test.reshape(-1,1)\n",
    "\n",
    "# fit model, plot line in red, calculate RMSE and R^2\n",
    "lin_reg = LinearRegression().fit(X_train, y_train) # the default parameters automatically check for an offset \n",
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "# %matplotlib notebook # this allows the plots to be interactive \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "pl1 = ax.scatter(X_train[:,0], X_train[:,1], y_train, zdir='z', s=20, c='black', depthshade=True)\n",
    "pl1 = ax.scatter(X_test[:,0], X_test[:,1], y_test, zdir='z', s=20, c='green', depthshade=True)\n",
    "xx1 = np.linspace(0, 100, 1000)\n",
    "xx2 = np.linspace(900,5100,1000)\n",
    "yy = w[0]*xx1+w[1]*xx2+b # fit line from model\n",
    "ax.plot(xx1,xx2,yy,c='red')\n",
    "plt.title('Fitting to fake BP data')\n",
    "ax.set_xlabel('Age',fontsize=12)\n",
    "ax.set_ylabel('Daily salt intake (mg)',fontsize=12)\n",
    "ax.set_zlabel('Blood Pressure (mmHg)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate error stats on the model\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "R2_train = r2_score(y_train,y_pred_train)\n",
    "RMSE_train = mean_squared_error(y_train, y_pred_train, squared=False) \n",
    "print('The R\\u00b2 value for this model on the training data is',R2_train)\n",
    "print('The RMSE value for this model on the training data is',RMSE_train)\n",
    "\n",
    "# add test data to plot in different color, calculate RMSE and R^2 \n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "R2_test = r2_score(y_test, y_pred_test) \n",
    "RMSE_test = mean_squared_error(y_test, y_pred_test, squared=False) \n",
    "print('\\nThe R\\u00b2 value for this model on the test data is',R2_test)\n",
    "print('The RMSE value for this model on the test data is',RMSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-commonwealth",
   "metadata": {},
   "source": [
    "***\n",
    "What if we want to determine which feature is more important? \n",
    "\n",
    "Let's look at our model weights as a crude proxy for feature importance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "print('Our model weights: ')\n",
    "print(w)\n",
    "print('remember the formula: Blood Pressure = w[0]*age+w[1]*salt intake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-indicator",
   "metadata": {},
   "source": [
    "What do we see? If we just look at the weights, we might come to the conclusion that age is contributing the most\n",
    "to the model, because its weight is ~31x greater than that for salt intake. \n",
    "\n",
    "But what are we not taking into consideration? The different distributions of our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(X_train[:,0])\n",
    "ax2.hist(X_train[:,1])\n",
    "ax1.set_title('Age (years)')\n",
    "ax2.set_title('Salt intake (mg/day)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-commission",
   "metadata": {},
   "source": [
    "As you can see, the age range is from 0-100 and the salt intake from 1000-5000. \n",
    "\n",
    "In order to use the weights to analyze feature importance, we need to standardize our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "# Standardization\n",
    "\n",
    "# split training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Fake_BP, test_size  = 0.2 , random_state=1)\n",
    "\n",
    "# format the arrays so that they will work with the algorithms properly\n",
    "X_train.reshape(-1,1)\n",
    "X_test.reshape(-1,1)\n",
    "y_train.reshape(-1,1)\n",
    "y_test.reshape(-1,1)\n",
    "\n",
    "\n",
    "# standardization (subtract the mean and divide by the variance - or just use this function)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-dividend",
   "metadata": {},
   "source": [
    "What are we doing and what aren't we doing? We are standardizing the training set, and then applying THE SAME\n",
    "transformation to the test set. Think about why this makes sense. You don't know what you don't know, and you don't know the exact distribution of your test data. \n",
    "\n",
    "We're also not normalizing the data (in this case). \n",
    "I won't go into the difference between standardization and normalization here. \n",
    "If you are interested, read up on it and think about when you would use which. \n",
    "\n",
    "What do our distributions look like now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(X_train[:,0])\n",
    "ax2.hist(X_train[:,1])\n",
    "ax1.set_title('Age (years)')\n",
    "ax2.set_title('Salt intake (mg/day)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-safety",
   "metadata": {},
   "source": [
    "Now let's use these standardized values in training a new model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # need this to close the interactive figure above\n",
    "\n",
    "# fit model, plot line in red, calculate RMSE and R^2\n",
    "lin_reg = LinearRegression().fit(X_train, y_train) # the default parameters automatically check for an offset \n",
    "w = lin_reg.coef_\n",
    "b = lin_reg.intercept_\n",
    "\n",
    "# Calculate error stats on the model\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "R2_train = r2_score(y_train,y_pred_train)\n",
    "RMSE_train = mean_squared_error(y_train, y_pred_train, squared=False) \n",
    "print('The R\\u00b2 value for this model on the training data is',R2_train)\n",
    "print('The RMSE value for this model on the training data is',RMSE_train)\n",
    "\n",
    "# add test data to plot in different color, calculate RMSE and R^2 \n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "R2_test = r2_score(y_test, y_pred_test) \n",
    "RMSE_test = mean_squared_error(y_test, y_pred_test, squared=False) \n",
    "print('\\nThe R\\u00b2 value for this model on the test data is',R2_test)\n",
    "print('The RMSE value for this model on the test data is',RMSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-questionnaire",
   "metadata": {},
   "source": [
    "Note that our R$^{2}$ and RMSE values did not change. \n",
    "\n",
    "What are our new weights? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at our model weights! \n",
    "print('\\nOur model weights: ')\n",
    "print(w)\n",
    "print('remember the formula: Blood Pressure = w[0]*age+w[1]*salt intake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-secretary",
   "metadata": {},
   "source": [
    "It looks like age and salt intake are pretty close to being equal contributors to the model, with salt intake being slightly more important. \n",
    "\n",
    "***\n",
    "\n",
    "Feature importance is extremely useful for someone who applies ML to biomedical problems. \n",
    "\n",
    "I highly encourage you to use models and methods that easily allow for feature importance estimates. Some models such as random forest will give you feature importance for free. At this point, watch out for models that completely obscure feature importance. Black box models are often harder to translate into medical practice.\n",
    "\n",
    "Because of the simple math that ordinary least squares performs, it is trying to force a relationship between all of the features and the output. \n",
    "\n",
    "You have to be careful with this. Sometimes a useless feature can end up with a really large coefficient and skew your model performance. There are other variants of linear regression that add a regularization step to limit the effects from garbage features. Common variants are Lasso, Ridge, and Elastic Net. Explore these on sklearn! You'll need to tune hyperparameters to use these. An example of how to do this is shared in the basic classification example. \n",
    "\n",
    "If you are brand new to regularization, I would recommend trying Elastic net as it is a combination of both Lasso and Ridge. \n",
    "\n",
    "Regularization for feature selection is also required when you have too many features and not enough examples. \n",
    "\n",
    "For now, play it safe and always try to have many more examples than features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
