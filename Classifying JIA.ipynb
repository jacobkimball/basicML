{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-threshold",
   "metadata": {},
   "source": [
    "## Classifying Juvenile Idiopathic Arthritis (JIA)\n",
    "\n",
    "Special thanks to Sevda Gharehbaghi and Kristy Scott Richardson for their help in putting together this code. \n",
    "\n",
    "JIA classification is an ongoing project in the <a href=\"http://www.irl.gatech.edu\" target=\"_blank\">Inan Research Lab</a>. \n",
    "\n",
    "Background on JIA, the protocol to collect the data, preprocessing information, and feature extraction details are shared in the following paper: \n",
    "\n",
    "<a href=\"https://www.frontiersin.org/articles/10.3389/fdgth.2020.571839/full\" target=\"_blank\">Knee Acoustic Emissions as a Digital Biomarker of Disease Status in Juvenile Idiopathic Arthritis</a> \n",
    "\n",
    "\n",
    "To summarize the dataset, 20 subjects with JIA and 18 healthy subjects had 4 miniature microphones attached to their knees during a short flexion extension exercise. Each subject repeated this exercise multiple times. Although choosing features and extracting them consistently across subjects is a very important part of the process, for the purposes of this notebook we will skip to importing the extracted feature set for analysis. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-cover",
   "metadata": {},
   "source": [
    "#### First step: import the important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat \n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-ordinance",
   "metadata": {},
   "source": [
    "#### Let's load the dataset and see what it looks like: (abbreviated)\n",
    "\n",
    "#### Note: if you are using Google Colab to run this notebook, you will need to upload 'JIA_data.csv' to your google drive and then mount your drive so you can access the data in Colab (and change the file path as necessary in the read_csv function). Please go through this tutorial to do learn how to do this: <a href=\"https://medium.com/@steve7an/how-to-test-jupyter-notebook-from-github-via-google-colab-7dc4b9b11a19\" target=\"_blank\">Google Colab/Github tutorial</a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "JIA_dataset = pd.read_csv('JIA_data.csv')\n",
    "\n",
    "data_top = JIA_dataset.head(); \n",
    "print(data_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-sculpture",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### It looks like we have 225 columns in this dataset. Let's print those out so we can see what we are working with. \n",
    "\n",
    "#### The full list of feature names is below. For more information on what these features are and how they were extracted, please look at the paper linked above and especially the supplementary table in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(JIA_dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-hardwood",
   "metadata": {},
   "source": [
    "#### As you scroll through these, you should see some repetition. After the first few columns, we have the mean, median and standard deviation of a set of 72 features. \n",
    "\n",
    "#### The first 9 columns aren't features. One of these values is the label! Here 0 means healthy, 1 means JIA. \n",
    "\n",
    "#### We'll have to remove these columns to create the actual feature matrix X and label vector y. But there may be some information in those other columns worth looking at, such as the mic number, cycle number or noise level. \n",
    "\n",
    "#### After we remove the columns, we'll print the abbreviated dataset again to make sure they are gone. \n",
    "\n",
    "#### We also pull the subject number for later use. In biomedical machine learning applications, you always need the subject number if you have more than one example per subject. DO NOT EVER FORGET THIS. EVER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = JIA_dataset.copy() \n",
    "cut_columns = ['Row','No','Subject','Label','Mic','Cycle No','Cycle Index_1','Cycle Index_2','NoiseLevel']\n",
    "X = data_copy.drop(cut_columns,axis=1)\n",
    "\n",
    "# let's make sure that those columns are gone \n",
    "X_top = X.head(); \n",
    "print(X_top)\n",
    "    \n",
    "y = JIA_dataset['Label'] # here 0 means healthy, 1 means JIA \n",
    "subject = JIA_dataset['Subject'] # DO NOT EVER FORGET THIS. EVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-lesson",
   "metadata": {},
   "source": [
    "#### Now let's take a peek at some of these features. \n",
    "\n",
    "#### Below we plot the kernel density estimation plot for one of the features. \n",
    "\n",
    "#### Replace 'variable_to_plot' with other column names to explore what other features look like. \n",
    "\n",
    "#### Remember that 'good' features will have separation left to right in the kde plot, and it doesn't always have to be linear separation (depending on your classifier choice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a feature on a kde plot\n",
    "\n",
    "JIA = X[y==1]\n",
    "healthy = X[y==0]\n",
    "\n",
    "variable_to_plot = 'ZCRMean' # choose different column names and see what the distributions look like! \n",
    "\n",
    "fig = plt.figure(figsize=(12,7)); # initialize your plot\n",
    "ax1 = sns.kdeplot(JIA[variable_to_plot]  ,\n",
    "    shade=True, color=\"xkcd:cherry\" , label=\"JIA\") # distribution of JIA subjects\n",
    "ax2 = sns.kdeplot(healthy[variable_to_plot]  ,\n",
    "    shade=True, color=\"xkcd:royal blue\" , label=\"healthy\") # distribution of healthy subjects\n",
    "\n",
    "# figure formatting\n",
    "plt.title('Distribution of JIA vs healthy subjects for ' + variable_to_plot)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.legend(loc=2, prop={'size': 16})\n",
    "ax1.grid(color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel(variable_to_plot ,  fontsize=20)\n",
    "plt.ylabel('Density', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-event",
   "metadata": {},
   "source": [
    "#### Let's double check the number of healthy and JIA subjects in this dataset, and check how many total examples we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many patients do we have with JIA? how many healthy? \n",
    "JIA_all = subject[y==1]\n",
    "healthy_all = subject[y==0]\n",
    "\n",
    "JIA_set = list(set(JIA_all))\n",
    "healthy_set = list(set(healthy_all))\n",
    "\n",
    "print('We have ' + str(len(JIA_set)) + ' JIA subjects, for a total of ' + str(len(JIA_all)) + ' recordings')\n",
    "print('We have ' + str(len(healthy_set)) + ' healthy subjects, for a total of ' + str(len(healthy_all)) + ' recordings' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-trunk",
   "metadata": {},
   "source": [
    "#### Let's figure out how many patients we want in the training and testing sets. \n",
    "\n",
    "#### For this size of dataset, I think it is appropriate to split it into 80% training, 20% testing. THIS IS NOT A HARD AND FAST RULE. Feel free to change the proportions and see what changes. \n",
    "\n",
    "#### We want the 80/20 split in both the JIA and healthy subjects so we don't randomly end up with all of the JIA patients in the training set and only healthy subjects in the test set, etc. This dataset is nice because we have roughly the same number of JIA and healthy patients, so we don't need to worry about the mismatched class size problem. \n",
    "\n",
    "#### Remember that we want to avoid the underdetermined case where M (number of features) is greater than N (number of examples) in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_N = len(JIA_all)+len(healthy_all) \n",
    "M = 216 # we have 216 features \n",
    "\n",
    "train_percentage = 0.8 \n",
    "\n",
    "n_train_JIA = round(train_percentage*len(JIA_set))\n",
    "n_train_healthy = round(train_percentage*len(healthy_set))\n",
    "\n",
    "print('(rough*) N to M ratio for the training set: '+str((total_N*train_percentage)/M))\n",
    "print('(*this is not exact because there are slight differences in the number of examples per subject)') \n",
    "print('As long as this percentage is greater than 1 we are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-mechanics",
   "metadata": {},
   "source": [
    "#### Now we randomly assign patients to training and testing sets based on the numbers of patients defined above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "JIA_train = random.sample(JIA_set, n_train_JIA) # choose n_train values from the set \n",
    "JIA_test = np.setdiff1d(JIA_set, JIA_train) # test set gets whatever is left \n",
    "\n",
    "healthy_train = random.sample(healthy_set, n_train_healthy) # choose n_train values from the set \n",
    "healthy_test = np.setdiff1d(healthy_set, healthy_train) # test set gets whatever is left \n",
    "\n",
    "# sanity check\n",
    "print(set(JIA_train).intersection(set(JIA_test)))\n",
    "print(set(healthy_train).intersection(set(healthy_test)))\n",
    "\n",
    "print('if these sets are both empty, there is no overlap between the training and testing sets')\n",
    "\n",
    "X_train = [] \n",
    "y_train = []\n",
    "\n",
    "# add healthy_train to training set X_train and training labels y_train\n",
    "for i in range(0,len(healthy_train)):\n",
    "    X_train.append(X[subject==healthy_train[i]])\n",
    "    y_train.append(y[subject==healthy_train[i]])\n",
    "    \n",
    "# add JIA_train to training set X_train and training labels y_train\n",
    "for i in range(0,len(JIA_train)):\n",
    "    X_train.append(X[subject==JIA_train[i]])\n",
    "    y_train.append(y[subject==JIA_train[i]])\n",
    "\n",
    "# make list of dataframes back into a single dataframe \n",
    "X_train = pd.concat(X_train) \n",
    "y_train = pd.concat(y_train)\n",
    "    \n",
    "X_test = [] \n",
    "y_test = []\n",
    "\n",
    "# add healthy_test to test set X_test and test labels y_test\n",
    "for i in range(0,len(healthy_test)):\n",
    "    X_test.append(X[subject==healthy_test[i]])\n",
    "    y_test.append(y[subject==healthy_test[i]])\n",
    "    \n",
    "# add JIA_test to test set X_test and test labels y_test\n",
    "for i in range(0,len(JIA_test)):\n",
    "    X_test.append(X[subject==JIA_test[i]])\n",
    "    y_test.append(y[subject==JIA_test[i]])\n",
    "\n",
    "# make list of dataframes back into a single dataframe \n",
    "X_test = pd.concat(X_test)\n",
    "y_test = pd.concat(y_test)\n",
    "\n",
    "print('\\nTraining and testing sets created:')\n",
    "print('X_train has dimensions: '+str(X_train.shape))\n",
    "print('y_train has dimensions: '+str(y_train.shape))\n",
    "print('X_test has dimensions: '+str(X_test.shape))\n",
    "print('y_test has dimensions: '+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-purse",
   "metadata": {},
   "source": [
    "#### And now we standardize the data. This makes it a lot easier for some classifiers to train a model. Remember that we standardize the training set and apply the same transformation to the testing set, simulating the case where we have a model in real life trained on  examples we have seen and testing on new unseen examples with an unknown distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization (subtract the mean and divide by the variance - or just use this function)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) # we apply the same transformation to X_test that we did to X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-officer",
   "metadata": {},
   "source": [
    "#### Note: if you are working with a different algorithm that requires hyperparameter tuning, you will need to further split your training set into smaller sets using a cross validation method such as leave-one-subject-out cross validation (LOSO-CV) so that you choose hyperparameter values only off the training data. \n",
    "\n",
    "#### We can skip this step since Logistic Regression doesn't have any hyperparameters to tune. \n",
    "\n",
    "#### Let's train our classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's where you actually train your classifier\n",
    "# set up logistic regression\n",
    "classifier = LogisticRegression(penalty = 'none', solver = 'sag', max_iter = 1000)\n",
    "\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-context",
   "metadata": {},
   "source": [
    "#### Now we test our classifier and report some performance metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance metrics \n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_test,y_pred)\n",
    "recall = sklearn.metrics.recall_score(y_test,y_pred)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "print('Accuracy is: ' + str(accuracy*100) + '%')\n",
    "print('Precision is: ' + str(precision*100) + '%')\n",
    "print('Sensitivity is: ' + str(recall*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-transmission",
   "metadata": {},
   "source": [
    "#### Note that every time you split new training and testing sets you will get different performance based on which subjects are randomly assigned to each set. Go back to where we split the datasets and run those cells again and see how the performance changes. \n",
    "\n",
    "#### Some people choose to combine their results over multiple runs to get a better sense of how their classifier performs on average. \n",
    "\n",
    "#### CAN YOU DO BETTER? Your challenges, should you choose to accept them: (can be done in any order) \n",
    "\n",
    "#### challenge 1: can you get better performance classifying by patient rather than by example? (This also makes a little more sense given the use case). How would you combine the example predictions to give an overall prediction for each subject?\n",
    "\n",
    "#### challenge 2: can you use a smaller dataset (choose your features carefully from those given) and get better performance? \n",
    "\n",
    "#### challenge 3: how does this compare to using logistic regression with regularization? (modify the 'penalty') \n",
    "\n",
    "#### challenge 4: can you use a different classifier and get better performance? (don't forget LOSO-CV if you tune hyperparameters!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
